{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2\n",
    "    Find all the mentions of world countries in the whole corpus, using the pycountry utility (HINT: remember that there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.) Perform sentiment analysis on every email message using the demo methods in the nltk.sentiment.util module. Aggregate the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level) that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo methods from the sentiment analysis module -- can you find substantial differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "print('Reading csv file...')\n",
    "df = pd.read_csv(path.join('hillary-clinton-emails', 'emails.csv'))\n",
    "print('is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "For this part, we only use the extracted body column. For instance, consider the case when Clinton replied to an email. The raw text contains all messages between them; however, the extracted body column contains only her reply. Hence, it makes sense to only consider the extracted body column to perform sentiment analysis of Hilary Clinton's comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = df['ExtractedBodyText'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "Cleaning the raw text is also crucial for further analysis. We use the following function to clean the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    # Convert all words to lower case\n",
    "    text = text.lower()    \n",
    "    # Tokenize the text while removing all words with less than 3 characters\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w{3,}')\n",
    "    token_text = tokenizer.tokenize(text)\n",
    "    # Remove common words from the text\n",
    "    s = set(nltk.corpus.stopwords.words('english'))\n",
    "    token_text = list(filter(lambda x: x not in s, token_text))\n",
    "    return ' '.join(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_text = raw_text.map(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "Now we perform the sentiment intensity analyzer using polarity score criteria on the clean text. We also use 2 different sentiment analyzer using from different demo modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "plr_scores = clean_text.map(sentim_analyzer.polarity_scores)\n",
    "compound_scores = plr_scores.map(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. demo_liu_hu_lexicon:\n",
    "Basic example of sentiment classification using Liu and Hu opinion lexicon. This function simply counts the number of positive, negative and neutral words in the sentence and classifies it depending on which polarity is more represented. Words that do not appear in the lexicon are considered as neutral. However, demo files can only print the results. Thus, we define a new function to store the results. The code is identical to NLTK sentiment codes available [here](http://www.nltk.org/_modules/nltk/sentiment/util.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo_liu_hu_lexicon(sentence):\n",
    "    \n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent))) # x axis for the plot\n",
    "    y = []\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            y.append(0) # neutral\n",
    "    \n",
    "    if pos_words > neg_words:\n",
    "        output = 'Positive'\n",
    "    elif pos_words < neg_words:\n",
    "        output = 'Negative'\n",
    "    elif pos_words == neg_words:\n",
    "        output = 'Neutral'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liu_hu_scores = clean_text.map(demo_liu_hu_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. demo_sent_subjectivity:\n",
    "Classify a single sentence as subjective or objective using a stored SentimentAnalyzer. Similar to the previous demo function, it can only print the results. Thus, we define a new function to store the results. The code is identical to NLTK sentiment codes available [here](http://www.nltk.org/_modules/nltk/sentiment/util.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo_sent_subjectivity(text):\n",
    "\n",
    "    from nltk.classify import NaiveBayesClassifier\n",
    "    from nltk.tokenize import regexp\n",
    "    from nltk.data import load\n",
    "    word_tokenizer = regexp.WhitespaceTokenizer()\n",
    "    try:\n",
    "        sentim_analyzer = load('sa_subjectivity.pickle')\n",
    "    except LookupError:\n",
    "        print('Cannot find the sentiment analyzer you want to load.')\n",
    "        print('Training a new one using NaiveBayesClassifier.')\n",
    "        sentim_analyzer = demo_subjectivity(NaiveBayesClassifier.train, True)\n",
    "\n",
    "    # Tokenize and convert to lower case\n",
    "    tokenized_text = [word.lower() for word in word_tokenizer.tokenize(text)]\n",
    "    return sentim_analyzer.classify(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_sub_scores = clean_text.map(demo_sent_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries mentioned in emails\n",
    "In this section, we find all countries which are mentioned in each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_dict = {country.alpha_2: [country.alpha_2.lower(),\n",
    "                                    country.alpha_3.lower(),\n",
    "                                    country.name.split(\",\")[0].lower()]\n",
    "                  for country in pycountry.countries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some countries names are still complex which should be shortened or modified. We add the following elements to our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_dict['GB'].extend(['uk', 'united kingdom', 'great britain'])\n",
    "countries_dict['US'].extend(['u.s.', 'u.s.a'])\n",
    "countries_dict['RU'].append('russia')\n",
    "countries_dict['KP'].append('north korea')\n",
    "countries_dict['KR'].append('south korea')\n",
    "countries_dict['SY'].append('syria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words in country alpha_2 and alpha_3 are misleading, e.g., are, pm, re, etc. We should exclude all these words from searching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_words = ['am', 'as', 'at', 'bf', 'cc', 'cv', 'ee', 'eh', 'gf', 'gg', 'id', 'co'\n",
    "                  'ie', 'im', 'in', 'is', 'it', 'no', 'np', 'pm', 'tf', 'to', 'us',\n",
    "                  'arm', 'can', 'com', 'col', 'mac', 'and', 'are', 'ago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_for_countries(text):\n",
    "    result = []\n",
    "    for key, values in countries_dict.items():\n",
    "        for value in values:\n",
    "            if len(value.split()) == 1:\n",
    "                if (value in text.split()) and (value not in excluded_words):\n",
    "                    result.append(key)\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if value in text:\n",
    "                    result.append(key)\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_lst = clean_text.map(search_for_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remaining"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
