{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/epfl_semester5/data_analyis/anaconda_folder/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "/Users/user/Desktop/epfl_semester5/data_analyis/anaconda_folder/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "#import pycountry\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re # for removing numbers\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from gensim import models,corpora\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get csv file\n",
    "df = pd.read_csv(path.join('hillary-clinton-emails', 'emails.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RawText contains all messages and we want to get the idea of the discussed topics we will use RawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739545\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER.\\nRELEASE IN FULL\\nFrom: Sullivan, Jacob J <Sullivan11@state.gov>\\nSent: Wednesday, September 12, 2012 10:16 AM\\nTo:\\nSubject: FW: Wow\\nFrom: Brose, Christian (Armed Services) (mailto:Christian_Brose@armed-servic,essenate.govi\\nSent: Wednesday, September 12, 2012 10:09 AM\\nTo: Sullivan, Jacob J\\nSubject: Wow\\nWhat a wonderful, strong and moving statement by your boss. please tell her how much Sen. McCain appreciated it. Me\\ntoo\\nUNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739545\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-5CB0045247\\n\\x0c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "df['RawText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_data=df['RawText']# useful_data=df['RawText']\n",
    "#Delete words with numbers\n",
    "without_num = [re.sub(r'\\d+', '', t) for t in useful_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(without_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize the data\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokennized_text=[tokenizer.tokenize(d) for d in without_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete words that are from stopwords and also the ones that are less than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopwords\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "#get rid of unnecessary words\n",
    "tokennized_text=[[s for s in t if (len(s)>2 and s not in sw)] for t in tokennized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert list of lists to one flattened list for easier data analysis steps below\n",
    "flattened = []\n",
    "for t in tokennized_text:\n",
    "    for w in t:\n",
    "        flattened.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('State', 29830), ('Department', 28674), ('UNCLASSIFIED', 26910), ('Date', 26826), ('Case', 26564), ('Doc', 26539), ('From', 19269), ('Sent', 18847), ('Subject', 18421), ('state', 12797), ('The', 11733), ('gov', 11346), ('RELEASE', 7951), ('Message', 7314), ('Original', 7244), ('com', 6675), ('would', 5684), ('Huma', 5681), ('said', 5579), ('Cheryl', 5531), ('Abedin', 5191), ('clintonemail', 4995), ('Mills', 4763), ('PART', 4347), ('Secretary', 4005), ('Sullivan', 3908), ('call', 3824), ('FULL', 3588), ('Obama', 3548), ('Jacob', 3463), ('also', 3452), ('one', 3442), ('time', 3370), ('government', 3251), ('Clinton', 3088), ('President', 3022), ('people', 2969), ('new', 2863), ('This', 2813), ('know', 2783), ('HDR', 2764), ('STATE', 2646), ('work', 2579), ('like', 2532), ('But', 2525), ('get', 2504), ('could', 2480), ('United', 2428), ('AbedinH', 2382), ('American', 2379), ('security', 2342), ('http', 2285), ('two', 2218), ('May', 2165), ('Friday', 2129), ('And', 2091), ('Thursday', 2075), ('support', 2064), ('Haiti', 2062), ('see', 2044), ('need', 2042), ('Sunday', 2039), ('Monday', 2033), ('well', 2025), ('States', 2016), ('today', 2014), ('MillsCD', 2000), ('Tuesday', 1976), ('last', 1959), ('Wednesday', 1951), ('hrod', 1950), ('September', 1927), ('think', 1916), ('first', 1898), ('want', 1844), ('SENSITIVE', 1823), ('Israel', 1806), ('back', 1806), ('world', 1805), ('country', 1774), ('policy', 1767), ('make', 1765), ('political', 1753), ('www', 1743), ('foreign', 1742), ('INFORMATION', 1725), ('meeting', 1725), ('SUBJECT', 1723), ('HOUSE', 1720), ('Saturday', 1707), ('BENGHAZI', 1705), ('Washington', 1703), ('FOIA', 1702), ('DEPT', 1700), ('AGREEMENT', 1698), ('REDACTIONS', 1696), ('SELECT', 1696), ('PRODUCED', 1696), ('COMM', 1696), ('WAIVER', 1696), ('week', 1684), ('House', 1675), ('They', 1648), ('year', 1636), ('may', 1634), ('much', 1625), ('told', 1615), ('way', 1603), ('good', 1602), ('years', 1591), ('many', 1590), ('August', 1589), ('public', 1583), ('Office', 1573), ('Minister', 1532), ('made', 1515), ('going', 1487), ('military', 1482), ('October', 1479), ('tomorrow', 1468), ('help', 1467), ('even', 1459), ('Sun', 1457), ('right', 1457), ('David', 1441), ('December', 1439), ('June', 1436), ('November', 1435), ('Sat', 1426), ('take', 1425), ('New', 1420), ('president', 1402), ('next', 1393), ('January', 1387), ('say', 1375), ('former', 1321), ('Hillary', 1319), ('Aug', 1313), ('long', 1306), ('Afghanistan', 1306), ('including', 1293), ('working', 1287), ('Iran', 1276), ('talk', 1275), ('speech', 1270), ('still', 1261), ('Pakistan', 1249), ('deal', 1243), ('There', 1235), ('countries', 1233), ('women', 1224), ('April', 1221), ('administration', 1213), ('email', 1210), ('China', 1208), ('Libya', 1199), ('report', 1187), ('day', 1183), ('called', 1179), ('issues', 1174), ('leaders', 1168), ('come', 1166), ('office', 1153), ('mailto', 1148), ('You', 1147), ('White', 1142), ('For', 1130), ('officials', 1122), ('international', 1122), ('March', 1103), ('Fri', 1095), ('says', 1093), ('Sid', 1090), ('July', 1087), ('That', 1084), ('Thu', 1084), ('asked', 1083), ('What', 1077), ('power', 1073), ('must', 1070), ('Lauren', 1067), ('Wed', 1062), ('issue', 1055), ('peace', 1054), ('America', 1047), ('efforts', 1038), ('Senate', 1037), ('best', 1036), ('part', 1036), ('morning', 1032), ('around', 1032), ('Israeli', 1029), ('She', 1029), ('since', 1018), ('Foreign', 1009), ('development', 1007), ('message', 1006), ('election', 997), ('Dec', 996), ('end', 991)]\n"
     ]
    }
   ],
   "source": [
    "# now check words with highest frequency to see whether some of the words arent from email content \n",
    "from collections import Counter\n",
    "counts = Counter(flattened)\n",
    "#print most common words\n",
    "print(counts.most_common(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By checking the most common words above we can see that around first 15 words are not from content of the messages in emails. These words are coming from email headers that doesn't represent the email content So, it would be safe to delete them for topic modeling of the conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get those top 15 words that arent useful for our purposes\n",
    "highest_frequency=counts.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unuseful_words=[]\n",
    "for i in range(0,len(highest_frequency)):\n",
    "    unuseful_words.append(highest_frequency[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State',\n",
       " 'Department',\n",
       " 'UNCLASSIFIED',\n",
       " 'Date',\n",
       " 'Case',\n",
       " 'Doc',\n",
       " 'From',\n",
       " 'Sent',\n",
       " 'Subject',\n",
       " 'state',\n",
       " 'The',\n",
       " 'gov',\n",
       " 'RELEASE',\n",
       " 'Message',\n",
       " 'Original',\n",
       " 'www',\n",
       " 'http',\n",
       " 'com',\n",
       " 'PART',\n",
       " 'FULL',\n",
       " 'HDR']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add other unuseful words from high frequency words that seems are out of context\n",
    "unuseful_words.append('www')\n",
    "unuseful_words.append('http')\n",
    "unuseful_words.append('com')\n",
    "unuseful_words.append('PART')\n",
    "unuseful_words.append('FULL')\n",
    "unuseful_words.append('HDR')\n",
    "#print unuseful words\n",
    "unuseful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update tokenized text after deleting the not useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokennized_text_n=[[s for s in t if s not in unuseful_words] for t in tokennized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete sentences(documents) with less than or equal to 5 words since they dont contain any useful information for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokennized_text_=[]\n",
    "for t in tokennized_text_n:\n",
    "    if(len(t)>5):\n",
    "        tokennized_text_.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for topic modelling from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get statistics about all tokens\n",
    "dictionary = corpora.Dictionary(tokennized_text_)\n",
    "#create a corpus of documents\n",
    "corpus = [dictionary.doc2bow(t) for t in tokennized_text_]\n",
    "#apply LDA model\n",
    "lda=models.LdaModel(corpus, num_topics=5,id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"said\" + 0.006*\"Obama\" + 0.004*\"would\" + 0.003*\"Clinton\" + 0.003*\"Haiti\" + 0.003*\"House\" + 0.002*\"President\" + 0.002*\"also\" + 0.002*\"time\" + 0.002*\"people\"'),\n",
       " (1,\n",
       "  '0.005*\"would\" + 0.004*\"government\" + 0.004*\"Israel\" + 0.003*\"one\" + 0.003*\"people\" + 0.003*\"American\" + 0.003*\"new\" + 0.003*\"said\" + 0.003*\"also\" + 0.003*\"United\"'),\n",
       " (2,\n",
       "  '0.008*\"Cheryl\" + 0.008*\"Mills\" + 0.006*\"Sullivan\" + 0.006*\"Jacob\" + 0.004*\"clintonemail\" + 0.004*\"would\" + 0.003*\"MillsCD\" + 0.003*\"said\" + 0.003*\"Feb\" + 0.003*\"Monday\"'),\n",
       " (3,\n",
       "  '0.012*\"Secretary\" + 0.008*\"Office\" + 0.005*\"Cheryl\" + 0.004*\"Mills\" + 0.003*\"Room\" + 0.003*\"President\" + 0.003*\"MEETING\" + 0.003*\"route\" + 0.003*\"ARRIVE\" + 0.002*\"DEPART\"'),\n",
       " (4,\n",
       "  '0.020*\"Huma\" + 0.018*\"Abedin\" + 0.014*\"call\" + 0.012*\"clintonemail\" + 0.008*\"AbedinH\" + 0.008*\"Sullivan\" + 0.006*\"Jacob\" + 0.005*\"hrod\" + 0.004*\"Sunday\" + 0.004*\"May\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"Lissa\" + 0.007*\"Muscatine\" + 0.006*\"Israel\" + 0.004*\"said\" + 0.003*\"Israeli\" + 0.003*\"American\" + 0.003*\"one\" + 0.003*\"draft\" + 0.002*\"also\" + 0.002*\"government\"'),\n",
       " (1,\n",
       "  '0.005*\"David\" + 0.004*\"would\" + 0.004*\"health\" + 0.003*\"family\" + 0.003*\"Friends\" + 0.003*\"Haiti\" + 0.003*\"time\" + 0.002*\"care\" + 0.002*\"letter\" + 0.002*\"Rio\"'),\n",
       " (2,\n",
       "  '0.011*\"SES\" + 0.011*\"Huma\" + 0.011*\"NEWS\" + 0.010*\"Abedin\" + 0.009*\"Reuters\" + 0.008*\"AbedinH\" + 0.007*\"O_Shift\" + 0.006*\"Mahogany\" + 0.006*\"said\" + 0.005*\"Cheryl\"'),\n",
       " (3,\n",
       "  '0.005*\"Germany\" + 0.003*\"freedom\" + 0.003*\"disc\" + 0.003*\"said\" + 0.003*\"world\" + 0.003*\"American\" + 0.003*\"new\" + 0.002*\"Europe\" + 0.002*\"history\" + 0.002*\"Berlin\"'),\n",
       " (4,\n",
       "  '0.011*\"Secretary\" + 0.009*\"Office\" + 0.005*\"Room\" + 0.004*\"MEETING\" + 0.004*\"route\" + 0.004*\"ARRIVE\" + 0.004*\"DEPART\" + 0.003*\"India\" + 0.003*\"Residence\" + 0.003*\"Private\"'),\n",
       " (5,\n",
       "  '0.006*\"would\" + 0.004*\"said\" + 0.004*\"one\" + 0.004*\"know\" + 0.003*\"get\" + 0.003*\"people\" + 0.003*\"could\" + 0.003*\"work\" + 0.003*\"Clinton\" + 0.003*\"women\"'),\n",
       " (6,\n",
       "  '0.018*\"Sullivan\" + 0.015*\"Jacob\" + 0.007*\"Secretary\" + 0.007*\"SullivanJJ\" + 0.006*\"clintonemail\" + 0.004*\"This\" + 0.004*\"Fri\" + 0.004*\"Office\" + 0.004*\"Huma\" + 0.003*\"message\"'),\n",
       " (7,\n",
       "  '0.006*\"sbwhoeop\" + 0.006*\"Koch\" + 0.004*\"Sid\" + 0.004*\"company\" + 0.003*\"government\" + 0.003*\"said\" + 0.003*\"Lib\" + 0.003*\"President\" + 0.003*\"political\" + 0.003*\"would\"'),\n",
       " (8,\n",
       "  '0.010*\"Israel\" + 0.008*\"said\" + 0.005*\"would\" + 0.005*\"Israeli\" + 0.004*\"peace\" + 0.004*\"Obama\" + 0.003*\"Palestinian\" + 0.003*\"Boehner\" + 0.003*\"United\" + 0.003*\"Clinton\"'),\n",
       " (9,\n",
       "  '0.014*\"clintonemail\" + 0.014*\"Lauren\" + 0.013*\"Feb\" + 0.013*\"Jiloty\" + 0.009*\"Huma\" + 0.009*\"JilotyLC\" + 0.008*\"Abedin\" + 0.007*\"Sullivan\" + 0.007*\"Anne\" + 0.007*\"Jacob\"'),\n",
       " (10,\n",
       "  '0.004*\"said\" + 0.004*\"government\" + 0.004*\"people\" + 0.004*\"Haiti\" + 0.004*\"United\" + 0.003*\"also\" + 0.003*\"Secretary\" + 0.003*\"Cheryl\" + 0.003*\"Pakistan\" + 0.003*\"Mills\"'),\n",
       " (11,\n",
       "  '0.008*\"Obama\" + 0.007*\"would\" + 0.004*\"But\" + 0.004*\"new\" + 0.004*\"said\" + 0.004*\"Afghanistan\" + 0.003*\"House\" + 0.003*\"one\" + 0.003*\"government\" + 0.003*\"president\"'),\n",
       " (12,\n",
       "  '0.023*\"Huma\" + 0.021*\"Abedin\" + 0.019*\"call\" + 0.016*\"clintonemail\" + 0.013*\"Cheryl\" + 0.010*\"Mills\" + 0.009*\"Sullivan\" + 0.009*\"AbedinH\" + 0.009*\"Jacob\" + 0.006*\"Sunday\"'),\n",
       " (13,\n",
       "  '0.011*\"Cheryl\" + 0.009*\"Mills\" + 0.008*\"Haiti\" + 0.004*\"May\" + 0.003*\"Meghann\" + 0.003*\"MillsCD\" + 0.003*\"This\" + 0.003*\"also\" + 0.003*\"GOH\" + 0.003*\"work\"'),\n",
       " (14,\n",
       "  '0.013*\"Mills\" + 0.012*\"Cheryl\" + 0.005*\"MillsCD\" + 0.005*\"Secretary\" + 0.005*\"Iran\" + 0.004*\"Wednesday\" + 0.004*\"McHale\" + 0.004*\"Jacob\" + 0.003*\"call\" + 0.003*\"Iranian\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example with many topics,e.g.15 topics\n",
    "lda=models.LdaModel(corpus, num_topics=15,id2word=dictionary)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "There is not any exact decisive rule to decide how many topics would be sufficient. From checking different numbers we come up to conclusion that 5 topics would be good enough. Increasing to more topics does not add very useful topic name content,but as above we show as an example choosing 15 topics also is not illogical."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
