{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/epfl_semester5/data_analyis/anaconda_folder/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "/Users/user/Desktop/epfl_semester5/data_analyis/anaconda_folder/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "#import pycountry\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re # for removing numbers\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from gensim import models,corpora\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path.join('hillary-clinton-emails', 'emails.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_data=df['RawText']\n",
    "#Delete words with numbers\n",
    "without_num = [re.sub(r'\\d+', '', t) for t in useful_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokennized_text=[tokenizer.tokenize(d) for d in without_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete words with less than 3 letters and also take care if it is from stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stopwords\n",
    "sw = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokennized_text=[[s for s in t if (len(s)>2 and s not in sw)] for t in tokennized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"The\" + 0.006*\"said\" + 0.006*\"State\" + 0.005*\"Department\" + 0.005*\"Obama\" + 0.005*\"UNCLASSIFIED\" + 0.004*\"Date\" + 0.004*\"Doc\" + 0.004*\"Case\" + 0.004*\"Israel\"'),\n",
       " (1,\n",
       "  '0.036*\"State\" + 0.036*\"Department\" + 0.034*\"UNCLASSIFIED\" + 0.033*\"Date\" + 0.033*\"Case\" + 0.032*\"Doc\" + 0.026*\"Sent\" + 0.025*\"Subject\" + 0.024*\"From\" + 0.018*\"state\"'),\n",
       " (2,\n",
       "  '0.013*\"Case\" + 0.012*\"Department\" + 0.012*\"Doc\" + 0.012*\"UNCLASSIFIED\" + 0.011*\"Date\" + 0.011*\"State\" + 0.008*\"From\" + 0.007*\"Sent\" + 0.007*\"The\" + 0.005*\"Subject\"'),\n",
       " (3,\n",
       "  '0.022*\"From\" + 0.020*\"Subject\" + 0.020*\"Sent\" + 0.016*\"State\" + 0.016*\"UNCLASSIFIED\" + 0.016*\"Date\" + 0.016*\"Case\" + 0.016*\"Doc\" + 0.015*\"Department\" + 0.013*\"gov\"'),\n",
       " (4,\n",
       "  '0.012*\"State\" + 0.011*\"Department\" + 0.010*\"Date\" + 0.010*\"Doc\" + 0.010*\"UNCLASSIFIED\" + 0.010*\"Case\" + 0.009*\"The\" + 0.004*\"would\" + 0.004*\"Subject\" + 0.003*\"From\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(tokennized_text)\n",
    "corpus = [dictionary.doc2bow(t) for t in tokennized_text]\n",
    "lda=models.LdaModel(corpus, num_topics=5,id2word=dictionary)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
